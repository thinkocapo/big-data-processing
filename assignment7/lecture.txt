COLLECTION TIER

PUSH VS PULL

Is data source looking for response from Collection Service?
sync vs async
Data Sourece <> Collection Service <> Target

"Data source success can depend on Collection Service giving back that ackwnoledgement"

PULL PATTERN would mean pulling data from databases. 
Example collector services - ETL tools, Scoop, Kafka connectors.
'data collection service knows about data source'
*Collection Service has control over how much data it can "swallow" in one Pull*
*in this case the Data Source is not-aware OF the Collection Service*
DataSource must maintain good record of all historical data
Collection Service requests it based on time-based criteria, or partition identifier, 
data in DataSource must be index.

PUSH PATTERN
Collection Service doesn't have to know much about the Data Source
DataSource decides how much data goes to CollectionService
Data source can be very lean - not keeping all historical data
DataSource must define the connection TO the collection service
web requests, IOT devices, examples.
*If the Collection Service goes down, then data is lost!*
DataSourece doesn't-care if Collection Tier even processes it correctly or not

vs.
STREAMING DATA COLLECTION
example data osources: Meetup, Tweeter Streams, Stocks (IEX) platform
Example Collector Services - Flume (via TCP source), Streaming systems (Spark streaming, Flink)

example - DataSource[Meetup] <streams..to> you the Collection Service
PUSH - all it knows is your TCP endpoints...
PULL - Data Source a priori knowledge (a list of) who is allowed to take data from it

"if your Collection Service goes down, it's not going to bring down Twitter that you're consuming from"


PUSH
Failures of the ds do affect the collection service
Failures of the collection services do not affect the data sources


FLUME - Fan-in
data locality. colocated writes on the same node between Flume/HDFS
4 agents collecting from hundreds of web serbers - can make bigger files for write to HDFS

airflow - workflow orchestration frameworks to help manage ^^ pipelines and flows

FLUME - Fan-out
Splitting on INgest - for disaster recovery....?
Flume Agent -> hdfs and also -> hdfs DR(disaster recovery)
Partitioning on Ingest - 
"topologies"
Multi-Agent pipeline
?

Flume Properties Files:
.capacity how many events can be stored in channel
'how many flume can read when its pulling from its source' .transactionCapacity
these only work for .type = memory


.type = logger <--- very useful for debugging
*alawys use a logger so you can debug any problem you're having*

can define multiple sinks a1.sinks = loggerSink, secondSink, thirdSink
Batch Size (also transactionCapacity for channels, for example) of 1 pull
"how many network transfers going to happen"




Different sources have different thread models - 

3 TYPES OF SOURCES TO POOL FROM
FILE, AVRO, SPOOLDIR
'place files to be ingested into a "spooling" directory on disk' e.g. logs from apps, FTP transfers. flume can keep reading them as they arrive.

CHANNELS
memory vs file based channel.
MEMORY CHANNEL: best performance but potential for data loss when agent goes down. RAM Limitation.
"connect more sources into same memory channel, so you can bundle events/data into there (e.g. once an hour event)"
Capacity - max # events that cane be stored. exceed this and the events are lost. non-durable, non-reliaable.
FILES CHANNEL: 
durable
less performant
periodic checkpoints
configure file channel to use multiple disks (separate directories)
SINKS:
transactional
one sink polls one channel only
"events will stay in the channel until the transaction is fully completed by the sink"
sometimes sink too slow

'yo'uoll have as many sinks as you think you need buckets'

SINKS: HDFS SINKS
write data into 'buckets' in HDFS

*avro source to hdfs sink*


-------
https://www.youtube.com/watch?v=8IKxf98h65Y
hi-throughput low-latency 