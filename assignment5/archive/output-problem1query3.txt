[hadoop@ip-172-31-27-205 ~]$ sudo spark-submit --master yarn problem1query3.py 
19/10/07 00:25:57 INFO SparkContext: Running Spark version 2.4.4
19/10/07 00:25:57 INFO SparkContext: Submitted application: problem1query3.py
19/10/07 00:25:57 INFO SecurityManager: Changing view acls to: root
19/10/07 00:25:57 INFO SecurityManager: Changing modify acls to: root
19/10/07 00:25:57 INFO SecurityManager: Changing view acls groups to: 
19/10/07 00:25:57 INFO SecurityManager: Changing modify acls groups to: 
19/10/07 00:25:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/10/07 00:25:58 INFO Utils: Successfully started service 'sparkDriver' on port 43195.
19/10/07 00:25:58 INFO SparkEnv: Registering MapOutputTracker
19/10/07 00:25:58 INFO SparkEnv: Registering BlockManagerMaster
19/10/07 00:25:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/10/07 00:25:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/10/07 00:25:58 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-3406ac48-5ab6-4d37-a682-085a9ab19797
19/10/07 00:25:58 INFO MemoryStore: MemoryStore started with capacity 424.4 MB
19/10/07 00:25:58 INFO SparkEnv: Registering OutputCommitCoordinator
19/10/07 00:25:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/10/07 00:25:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-27-205.us-east-2.compute.internal:4040
19/10/07 00:25:59 INFO Executor: Starting executor ID driver on host localhost
19/10/07 00:25:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45013.
19/10/07 00:25:59 INFO NettyBlockTransferService: Server created on ip-172-31-27-205.us-east-2.compute.internal:45013
19/10/07 00:25:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/10/07 00:25:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-27-205.us-east-2.compute.internal, 45013, None)
19/10/07 00:25:59 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-27-205.us-east-2.compute.internal:45013 with 424.4 MB RAM, BlockManagerId(driver, ip-172-31-27-205.us-east-2.compute.internal, 45013, None)
19/10/07 00:25:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-27-205.us-east-2.compute.internal, 45013, None)
19/10/07 00:25:59 INFO BlockManager: external shuffle service port = 7337
19/10/07 00:25:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-27-205.us-east-2.compute.internal, 45013, None)
19/10/07 00:26:03 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/local-1570407958985
19/10/07 00:26:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 237.6 KB, free 424.2 MB)
19/10/07 00:26:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.1 KB, free 424.2 MB)
19/10/07 00:26:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-27-205.us-east-2.compute.internal:45013 (size: 24.1 KB, free: 424.4 MB)
19/10/07 00:26:04 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
19/10/07 00:26:04 INFO GPLNativeCodeLoader: Loaded native gpl library
19/10/07 00:26:04 INFO LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 082e98ac708024722b9a71933768ecad2e086acc]
19/10/07 00:26:05 INFO FileInputFormat: Total input files to process : 4
19/10/07 00:26:05 INFO SparkContext: Starting job: countByKey at /home/hadoop/problem1query3.py:46
19/10/07 00:26:05 INFO DAGScheduler: Got job 0 (countByKey at /home/hadoop/problem1query3.py:46) with 4 output partitions
19/10/07 00:26:06 INFO DAGScheduler: Final stage: ResultStage 0 (countByKey at /home/hadoop/problem1query3.py:46)
19/10/07 00:26:06 INFO DAGScheduler: Parents of final stage: List()
19/10/07 00:26:06 INFO DAGScheduler: Missing parents: List()
19/10/07 00:26:06 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at countByKey at /home/hadoop/problem1query3.py:46), which has no missing parents
19/10/07 00:26:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KB, free 424.2 MB)
19/10/07 00:26:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KB, free 424.2 MB)
19/10/07 00:26:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-27-205.us-east-2.compute.internal:45013 (size: 5.4 KB, free: 424.4 MB)
19/10/07 00:26:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1201
19/10/07 00:26:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (PythonRDD[2] at countByKey at /home/hadoop/problem1query3.py:46) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
19/10/07 00:26:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
19/10/07 00:26:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7913 bytes)
19/10/07 00:26:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7913 bytes)
19/10/07 00:26:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 7913 bytes)
19/10/07 00:26:06 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 7913 bytes)
19/10/07 00:26:06 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
19/10/07 00:26:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
19/10/07 00:26:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
19/10/07 00:26:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
19/10/07 00:26:06 INFO HadoopRDD: Input split: s3a://inputfilesassignment4/wordcount/file-input1.csv:0+1208295
19/10/07 00:26:06 INFO HadoopRDD: Input split: s3a://inputfilesassignment4/wordcount/file-input4.csv:0+1208094
19/10/07 00:26:06 INFO HadoopRDD: Input split: s3a://inputfilesassignment4/wordcount/file-input2.csv:0+1208086
19/10/07 00:26:06 INFO HadoopRDD: Input split: s3a://inputfilesassignment4/wordcount/file-input3.csv:0+1208251
19/10/07 00:26:08 INFO PythonRunner: Times: total = 1850, boot = 299, init = 115, finish = 1436
19/10/07 00:26:08 INFO PythonRunner: Times: total = 1894, boot = 307, init = 124, finish = 1463
19/10/07 00:26:08 INFO PythonRunner: Times: total = 1929, boot = 369, init = 155, finish = 1405
19/10/07 00:26:08 INFO PythonRunner: Times: total = 1955, boot = 353, init = 141, finish = 1461
19/10/07 00:26:08 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 385212 bytes result sent to driver
19/10/07 00:26:08 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 383188 bytes result sent to driver
19/10/07 00:26:08 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 387556 bytes result sent to driver
19/10/07 00:26:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 384467 bytes result sent to driver
19/10/07 00:26:08 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2322 ms on localhost (executor driver) (1/4)
19/10/07 00:26:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2350 ms on localhost (executor driver) (2/4)
19/10/07 00:26:08 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2320 ms on localhost (executor driver) (3/4)
19/10/07 00:26:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2375 ms on localhost (executor driver) (4/4)
19/10/07 00:26:08 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39957
19/10/07 00:26:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
19/10/07 00:26:08 INFO DAGScheduler: ResultStage 0 (countByKey at /home/hadoop/problem1query3.py:46) finished in 2.521 s
19/10/07 00:26:08 INFO DAGScheduler: Job 0 finished: countByKey at /home/hadoop/problem1query3.py:46, took 2.727838 s
19/10/07 00:26:08 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-27-205.us-east-2.compute.internal:4040
19/10/07 00:26:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/10/07 00:26:08 INFO MemoryStore: MemoryStore cleared
19/10/07 00:26:08 INFO BlockManager: BlockManager stopped
19/10/07 00:26:08 INFO BlockManagerMaster: BlockManagerMaster stopped
19/10/07 00:26:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/10/07 00:26:08 INFO SparkContext: Successfully stopped SparkContext
19/10/07 00:26:09 INFO ShutdownHookManager: Shutdown hook called
19/10/07 00:26:09 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-431843ad-f2df-44d4-943b-f25946538aea
19/10/07 00:26:09 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-ac177002-add5-42a1-b55b-3aba80c6cb57/pyspark-035aea45-a84b-4f2b-b499-5c2277a5b219
19/10/07 00:26:09 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-ac177002-add5-42a1-b55b-3aba80c6cb57
[hadoop@ip-172-31-27-205 ~]$ 
