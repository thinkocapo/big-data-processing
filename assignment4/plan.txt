--------------------- RESOURCES --------------------
Hue
think
1A

Technically what is the difference between s3n, s3a and s3?
https://stackoverflow.com/questions/33356041/technically-what-is-the-difference-between-s3n-s3a-and-s3

----------------------- PROBLEMS --------------------

PROBLEM 1 STEPS
1. Upload 4 input log files to S3 bucket inputfilesassignment4
+ file-input1.csv Object URL
https://inputfilesassignment4.s3.us-east-2.amazonaws.com/file-input1.csv
+ with a folder it's shown in the URL:
https://inputfilesassignment4.s3.us-east-2.amazonaws.com/wordcount/file-input1.csv

2. Example wordcount job w/ s3 data
+ ssh into master node,
ssh -i ~/assignment2.pem hadoop@ec2-18-218-82-247.us-east-2.compute.amazonaws.com
+ find /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar
  + find the wordcount program 'bundled example' <--- assume its in the .jar
  need put the wordcount mappers reducers on the slave nodes?

- how to reference object URL or something like s3client.get(<name_of_file.csv)
"In general, when specifying a path to S3, we will follow this required convention: `s3a://bucket-name/directory/`"
...so trying:
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv output_wordcount.txt

s3://inputfilesassignment4/file-input1.csv

+ run hadoop wordcount job
hadoop jar <name.jar> <class_in_jar> <input_file> <output_file> at the end.
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv output_wordcount.txt
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3://inputfilesassignment4/file-input1.csv output_wordcount.txt
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv wordcount
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv s3a://inputfilesassignment4/wordcount

YESSS
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -ls
Found 2 items
drwxr-xr-x   - hadoop hadoop          0 2019-09-26 03:46 output_wordcount.txt
drwxr-xr-x   - hadoop hadoop          0 2019-09-26 03:57 output_wordcount2.txt

[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00001
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00002
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00003
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00004

+ write it to S3
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv s3a://inputfilesassignment4/wordcount
+ find how to monitor the # of splits, map, reduce tasks for this job (review lab video if can't find it through console.aws.amazon.com GUI)
[SCREENSHOTS OF LOGS]


PROBLEM2 STEPS
+ ssh into slave nodes and give security permissions <-- looked like it was already done

9:47 NEXT:
- review all steps and...
- write csv's to hdfs (review video, lab slides)

- write my mapper + reducer py's <--- understand the lab ones first and model off of that
- scp them to each slave node
- figure out how to pass 4 csv's (from hdfs!?) all at once
