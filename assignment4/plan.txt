--------------------- RESOURCES --------------------
Hue
think
1A

Technically what is the difference between s3n, s3a and s3?
https://stackoverflow.com/questions/33356041/technically-what-is-the-difference-between-s3n-s3a-and-s3

----------------------- PROBLEMS --------------------

PROBLEM 1 STEPS
1. Upload 4 input log files to S3 bucket inputfilesassignment4
+ file-input1.csv Object URL
https://inputfilesassignment4.s3.us-east-2.amazonaws.com/file-input1.csv
+ with a folder it's shown in the URL:
https://inputfilesassignment4.s3.us-east-2.amazonaws.com/wordcount/file-input1.csv

2. Example wordcount job w/ s3 data
+ ssh into master node,
ssh -i ~/assignment2.pem hadoop@ec2-<ip_ad_dress_here>.us-east-2.compute.amazonaws.com
+ find /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar
  + find the wordcount program 'bundled example' <--- assume its in the .jar
  need put the wordcount mappers reducers on the slave nodes?

- how to reference object URL or something like s3client.get(<name_of_file.csv)
"In general, when specifying a path to S3, we will follow this required convention: `s3a://bucket-name/directory/`"
...so trying:
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv output_wordcount.txt

s3://inputfilesassignment4/file-input1.csv

+ run hadoop wordcount job
hadoop jar <name.jar> <class_in_jar> <input_file> <output_file> at the end.
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv output_wordcount.txt
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3://inputfilesassignment4/file-input1.csv output_wordcount.txt
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv wordcount
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv s3a://inputfilesassignment4/wordcount

YESSS
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -ls
Found 2 items
drwxr-xr-x   - hadoop hadoop          0 2019-09-26 03:46 output_wordcount.txt
drwxr-xr-x   - hadoop hadoop          0 2019-09-26 03:57 output_wordcount2.txt

[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00001
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00002
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00003
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00004

+ write it to S3
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv s3a://inputfilesassignment4/wordcount
+ find how to monitor the # of splits, map, reduce tasks for this job (review lab video if can't find it through console.aws.amazon.com GUI)
[SCREENSHOTS OF LOGS]


PROBLEM2 STEPS
+ ssh into slave nodes and give security permissions <-- looked like it was already done
+ figure out how mapper 'prints' or 'writes' to something the reducer can get by 'line in fileinput.input()''
+ write my mapper + reducer py's, print statements with \t involved


- scp them to each slave node
- scp them to master node? try without this...

- figure out how to pass 4 csv's (from hdfs!?) all at once <---by -input s3://cscie88-amitathex/input/



PROBLEM2 IN JAVA
- is it worth re-writing out my mapper/reudcer in java? NO
- just ask about differences instead
- jar with 1 class, try it
- could try just the wordcount example

"""
Hello, I have a few questions on producing my own 'jar' with mappers and reducers written in java.



Is the count_jobs.jar contains only the HoursCounter_t class, which then has mapper and reducer subclasses in it, in HoursCounter_t.java?



hadoop jar count_jobs.jar org.cscie88.hadoopmr.HoursCounter input_logs output_hr
Or do I need a pom.xml and Maven to help me add dependenices for hadoop? The command 'hadoop' is already taking care of running hadoop for me, correct?



When I was reading about uploading to S3 I found this hadoop package called S3A and you needed to include in in your Maven build, so I was thinking I had to use Maven. But it turned out this tool was not even necessary. https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#Introducing_the_Hadoop_S3A_client.


But if you need to add in additional features to Hadoop and you do that by adding in more specialized Jars, then is it still the same setup regardless of whether your mappers are in java vs python? After all the start of the command is 'hadoop jar' and its written in java. In the python example you call '/usr/lib/hadoop'. Can ssh into the Node and update config from there?
"""


QUESTION - Python vs JAVA
if just stdin/stdout then Scala would work same way?
if python functionality added to appease them, then the heck with it.
size of arrays
data types, 'stdin/stdout' enough? no strong typing. escape chars, unicode, unsigned_int, maybe java better
errors in ETL, which are more readable? i'd rather deal with java errors.
adding more hadoop packages/functionality is still just by adding more java jars, and mapper/reducer can stay in python?
what's more common in data world? is java that painful?
and....

09/26
Namenodes is bottleneck and 10-100 PB's data
so use FSView, read HDFS Federation



DATA QUALITY
To enhance data quality, we identified two key areas for improvement. First, we want to avoid non-schema-conforming data when some of the upstream data stores do not mandatorily enforce or check data schema before storage (e.g., storing a key-value where the value is a JSON blob). This results in bad data entering our Hadoop ecosystem, thereby affecting all downstream users also relying on this data. To prevent an influx of bad data, we are transitioning all upstream data stores towards performing mandatory schema checks on data content and rejecting data entries if there are any issues (e.g., not confirming with the schema) with the data.


The second area that we found problematic was the quality of the actual data content. While using schemas ensures that data contains correct data types, they do not check the actual data values (e.g., an integer as opposed to a positive number between [0,150]). To improve data quality, we are expanding our schema service to support semantic checks. These semantic checks (in other words, Uber-specific data types) allows us to add extra constraints on the actual data content beyond basic structural type checking.

DATA EFFICIENCY
Data efficiency
To improve data efficiency, we are moving away from relying on dedicated hardware for any of our services and towards service dockerization. In addition, we are unifying all of our resource schedulers within and across our Hadoop ecosystem to bridge the gap between our Hadoop and non-data services across the company. This allows all jobs and services to be scheduled in a unified fashion regardless of the medium it will be executed in. As Uber grows, data locality will be a big concern for Hadoop applications, and a successful unified resource manager can bring together all existing schedulers (i.e., Yarn, Mesos, and Myriad).

Python:
/user/bin/lib/hadoop - there's config file, how to use additional jars?
vs
Java:

maven pom.xml?

as long as writes stdout , can use any lang? scala?

hadoop jar /path/<my_custom_jar>.jar? vs config files? both?

java vs python https://www.quora.com/Which-programming-language-is-good-to-drive-Hadoop-and-Spark-Java-Python-or-Scala



https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/ViewFs.html ?


How to 'restart' the cluster? can't ssh into it to run a command to do so, and don't see it in ec2's list?