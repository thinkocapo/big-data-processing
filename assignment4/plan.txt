--------------------- RESOURCES --------------------
Hue
think
1A

Technically what is the difference between s3n, s3a and s3?
https://stackoverflow.com/questions/33356041/technically-what-is-the-difference-between-s3n-s3a-and-s3

----------------------- PROBLEMS --------------------

PROBLEM 1 STEPS
1. Upload 4 input log files to S3 bucket inputfilesassignment4
+ file-input1.csv Object URL
https://inputfilesassignment4.s3.us-east-2.amazonaws.com/file-input1.csv
+ with a folder it's shown in the URL:
https://inputfilesassignment4.s3.us-east-2.amazonaws.com/wordcount/file-input1.csv

2. Example wordcount job w/ s3 data
+ ssh into master node,
ssh -i ~/assignment2.pem hadoop@ec2-18-218-82-247.us-east-2.compute.amazonaws.com
+ find /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar
  + find the wordcount program 'bundled example' <--- assume its in the .jar
  need put the wordcount mappers reducers on the slave nodes?

- how to reference object URL or something like s3client.get(<name_of_file.csv)
"In general, when specifying a path to S3, we will follow this required convention: `s3a://bucket-name/directory/`"
...so trying:
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv output_wordcount.txt

s3://inputfilesassignment4/file-input1.csv

+ run hadoop wordcount job
hadoop jar <name.jar> <class_in_jar> <input_file> <output_file> at the end.
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv output_wordcount.txt
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3://inputfilesassignment4/file-input1.csv output_wordcount.txt
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv wordcount
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv s3a://inputfilesassignment4/wordcount

YESSS
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -ls
Found 2 items
drwxr-xr-x   - hadoop hadoop          0 2019-09-26 03:46 output_wordcount.txt
drwxr-xr-x   - hadoop hadoop          0 2019-09-26 03:57 output_wordcount2.txt

[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00001
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00002
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00003
[hadoop@ip-172-31-25-109 ~]$ hadoop fs -cat output_wordcount.txt/part-r-00004

+ write it to S3
hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount s3a://inputfilesassignment4/file-input1.csv s3a://inputfilesassignment4/wordcount
+ find how to monitor the # of splits, map, reduce tasks for this job (review lab video if can't find it through console.aws.amazon.com GUI)
[SCREENSHOTS OF LOGS]


PROBLEM2 STEPS
+ ssh into slave nodes and give security permissions <-- looked like it was already done




12:27 NEXT:
+ figure out how mapper 'prints' or 'writes' to something the reducer can get by 'line in fileinput.input()''
- write my mapper + reducer py's <--- all you need is print statements with \t involved
- scp them to each slave node
- figure out how to pass 4 csv's (from hdfs!?) all at once <---by -input s3://cscie88-amitathex/input/



PROBLEM2 IN JAVA
- is it worth re-writing out my mapper/reudcer in java? NO
- just ask about differences instead
- jar with 1 class, try it
- could try just the wordcount example

"""
Hello, I have a few questions on producing my own 'jar' with mappers and reducers written in java.



Is the count_jobs.jar contains only the HoursCounter_t class, which then has mapper and reducer subclasses in it, in HoursCounter_t.java?



hadoop jar count_jobs.jar org.cscie88.hadoopmr.HoursCounter input_logs output_hr
Or do I need a pom.xml and Maven to help me add dependenices for hadoop? The command 'hadoop' is already taking care of running hadoop for me, correct?



When I was reading about uploading to S3 I found this hadoop package called S3A and you needed to include in in your Maven build, so I was thinking I had to use Maven. But it turned out this tool was not even necessary. https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html#Introducing_the_Hadoop_S3A_client.


But if you need to add in additional features to Hadoop and you do that by adding in more specialized Jars, then is it still the same setup regardless of whether your mappers are in java vs python? After all the start of the command is 'hadoop jar' and its written in java. In the python example you call '/usr/lib/hadoop'. Can ssh into the Node and update config from there?
"""


QUESTION - Python vs JAVA
if just stdin/stdout then Scala would work same way?
if python functionality added to appease them, then the heck with it.
size of arrays
data types, 'stdin/stdout' enough? no strong typing. escape chars, unicode, unsigned_int, maybe java better
errors in ETL, which are more readable? i'd rather deal with java errors.
adding more hadoop packages/functionality is still just by adding more java jars, and mapper/reducer can stay in python?
what's more common in data world? is java that painful?
