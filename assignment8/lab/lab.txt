bin/zookeper-server-start.sh config/zookerper.properties at 4:20p
****LOCAL RUN ******

1 download kafka files, don't have to install them. it's bin (see shell scripts)
directory called kafka_2.11-2.2.1

2 start zookeper 
3 verify zookeepr is running - nc -vz localhost 2181 is zookeeper port
4 start kafka
bin/kafaka-server-start.sh config/server.properties 7:04p
^^is one-node cluster
log activity in oboth panes

kafka runs in 9092
nv -vc localhost 9092

'kafka ships with zookepr, it was in same kafka_2.11-2.2.1/bin/zookeepr... directory
"always stop KAFKA first THEN ZOOKEEPER"

ps -eaf grep | java
sudo kill -9 25073

^^ kafka is now stopped.

5 START KAFKA CLUSTER

****SWITCH TO DOCKER******

6 11:53
.yaml
ports: - "2181:2181" don't realy need because we're executing things inside of the container on kafka, don't need to expose it
'environment variables that container exposes'
`HOSTNAME_COMMAND
PORTS: - 9092 "makes it run on 9092, but does not expose it" "don't try expose it 9092:9092 because all nodes will be listening on 9092"

'one more containre to run the python example...'
python-example: depends_on: -fafka -zookeeper
'volumes: - "./python:/python" makes my machine's code available to the d containers....'

7 start docker...
# single-node kafka cluster
docker-compose -f kafka-docker-compose.yaml up -d
docker ps -a
(see 1 zookeepr, 1 kafka node, 1 python container)

'3 node cluster using teh scale option in docker-compose'
docker-compose -f kafka-docker-compose.yaml up -d --scale kafka=3
*WOOOAHH* so `--scale kafka=3` makes 3 of them
docker ps -a
(see 3 kafkas running)

8 18:29 connect to a running Kafka container
docker exec -it <NAME of container> bash
"it didnt matter which node we exec'd (connected) to"

9 let's try create a topic
kafka-topics.sh





20:22
bash-4.4# cd /opt/kafka/bin
connect-distributed.sh               kafka-consumer-perf-test.sh          kafka-reassign-partitions.sh         kafka-verifiable-producer.sh
connect-standalone.sh                kafka-delegation-tokens.sh           kafka-replica-verification.sh        trogdor.sh
kafka-acls.sh                        kafka-delete-records.sh              kafka-run-class.sh                   windows
kafka-broker-api-versions.sh         kafka-dump-log.sh                    kafka-server-start.sh                zookeeper-security-migration.sh
kafka-configs.sh                     kafka-log-dirs.sh                    kafka-server-stop.sh                 zookeeper-server-start.sh
kafka-console-consumer.sh            kafka-mirror-maker.sh                kafka-streams-application-reset.sh   zookeeper-server-stop.sh
kafka-console-producer.sh            kafka-preferred-replica-election.sh  kafka-topics.sh                      zookeeper-shell.sh
kafka-consumer-groups.sh             kafka-producer-perf-test.sh          kafka-verifiable-consumer.sh

22:26
--describe <topic name optional>

"The Producer specifies the ackwnoedlgement e.g. 'at least from 2 machines'"



kafka-console-consumer.sh --broker-list localhost:9092 --topic will_topic --property "parse.key=true" --property "key.separator=:"
 ?

 kafka-console-consumer.sh --bottstrap-server localhost:9092 --topic will_topic




"consumer and producers don't have to be on same machine as Kafka Server/Nodes/CLuser, as long as have binaries and port of the Cluster you can point to that"

kafka-console-producer.sh \
  --broker-list localhost:9092 \
  --topic will_topic \
  --property "parse.key=true" \
  --property "key.separator=:"



*"These kafka-console* shell scripts are just for out-of-the-box"*


# does not jus talk to 1 machine, talks to all [represented]
producer = KafkaProducer(bootstrap_servers='kafka:9092')








*HOW TO STOP ALL IN OPERATION*
docker-compose -f kafka-docker-compose.yaml down -d
11:01a put the consumer producer py's in /python so .yaml's VOLUMEs will find them, put to container

*HOW TO START AGAIN*
docker-compose -f kafka-docker-compose.yaml up -d --scale kafka=3

*DON'T FORGET KAFKA-TOPIC*
kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 2 --partitions 5 --topic test_topic

*GET INTO PYTHON IMAGE CONTAINER - producer*
 $ docker exec -it assignment8_python-example_1 /bin/sh
python /python/python-producer.py

*GET INTO PYTHON CONTAINER - CONSUMER*
 $ docker exec -it assignment8_python-example_1 python ./python/python-consumer.py


47:minute


kafka server poroperties in /opt/kafka/config/server.properties
# has the broker.id
# this gets shown by the --describe command
# log.dirs
log.dirs=/kafka/kafka-logs-a866c90bc301

or
bash-4.4# cd /opt/kafka/config/
bash-4.4# cat server.properties | grep .dir
# A comma separated list of directories under which to store log files
log.dirs=/kafka/kafka-logs-a866c90bc301


58:minute
/opt/kafka/config/server.properties
advertised.host.name=172.27.0.4
^^ import if machines outside the private network trying to access it. should always be a public IP

/opt/kafka/config/log4j.properties dictates where the log files go
/opt/kafka/bin/zookeeper-shell.sh

/zookeeper-shell.sh zookeeper:2181
ls /
ls /brokers/ids
# shows [1003, 1002, 1001]  "3 [kafka] nodes in the Cluster]"
*"and the IP address of each of those 3 is in that broker"*

get /brokers/ids/1001
{"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://172.24.0.3:9092"],"jmx_port":-1,"host":"172.24.0.3","timestamp":"1572115711206","port":9092,"version":4}


get /brokers/ids/1002
{"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://172.24.0.4:9092"],"jmx_port":-1,"host":"172.24.0.4","timestamp":"1572115712153","port":9092,"version":4}

^^IP 172.24.0.4 comes from server.properties

1 client talks to node
2 ndoe talks to zookeeper to get names/IPs of all other nodes
3 sends it back to client




1:04:00
when producer produces message
it ends up in:
/kafka/kafka-logs-a866c90bc301
and in here...
00000000.log file is binary but has the messages...
each of these is a different partition for the test_topic"
kafka-run-class.sh kafka.tools.DumpLogSegments --files /kafka/kafka-logs-a866c90bc301/test_topic-0/00000000000000000000.log --print-data-log --deep-iteration | less
vs
kafka-run-class.sh kafka.tools.DumpLogSegments --files /kafka/kafka-logs-a866c90bc301/test_topic-1/00000000000000000000.log --print-data-log --deep-iteration | less
vs

the actual payload data [log] not the application log
is why we see 'key:5 payload: 5'
FINALLYyyy
"key:5 is mapped to this partition, that's why all with '5' go to this partition"...


1:12:19
List all the Consumer Groups
bash-4.4# kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
consumer-1

bash-4.4# kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group consumer-1 --describe

Consumer group 'consumer-1' has no active members.
GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
consumer-1      test_topic      0          18              18              0               -               -               -
consumer-1      test_topic      4          63              63              0               -               -               -
consumer-1      test_topic      2          25              25              0               -               -               -
consumer-1      test_topic      1          61              61              0               -               -               -
consumer-1      test_topic      3          110             110             0               -               -               -

SO....

start up a python-consumer.py again from python container and re-execute:
bash-4.4# kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group consumer-1 --describe

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                             HOST            CLIENT-ID
consumer-1      test_topic      0          18              18              0               kafka-python-1.3.5-8ef9ad9d-afb7-49b5-be1c-b00c47428b63 /172.24.0.6     kafka-python-1.3.5
consumer-1      test_topic      4          63              63              0               kafka-python-1.3.5-8ef9ad9d-afb7-49b5-be1c-b00c47428b63 /172.24.0.6     kafka-python-1.3.5
consumer-1      test_topic      2          25              25              0               kafka-python-1.3.5-8ef9ad9d-afb7-49b5-be1c-b00c47428b63 /172.24.0.6     kafka-python-1.3.5
consumer-1      test_topic      1          61              61              0               kafka-python-1.3.5-8ef9ad9d-afb7-49b5-be1c-b00c47428b63 /172.24.0.6     kafka-python-1.3.5
consumer-1      test_topic      3          110             110             0               kafka-python-1.3.5-8ef9ad9d-afb7-49b5-be1c-b00c47428b63 /172.24.0.6     kafka-python-1.3.5
bash-4.4# 
:)
^^ host is same because it's only 1 consumer python we're running


"LOG-END-OFFSET tells you Where the Producers are - how many messages produced and are available in Kafka"
"CURRENT-OFFSET is the specific consumer we started, it has 'consumed until this offset...'
271 272 LAG Of 1 means its only consumed 271
"if Lag is 0 then we know consumer has consumed all of the messages"
"* can find slow consumers this way *" ;)

1:15:07
GROUP Is consumer-1 because 1 consumer consuming from all partitions
w/ client-id kafka-python-1.3.5 too



docker exec -it assignment8_python-example_1 python python/python-producer.py
docker exec -it assignment8_python-example_1 python python/python-consumer.py





"use two different consumerGroupings and they'll work off of different offsets"

1:28:43
asks about Flume. nothing to show.
Q&A time



9:30p
bash-4.4# kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 2 --partitions 2 --topic problem2
Created topic problem2.


1 put the index.html in there
2 copy back in my apache flume config file
3 python curl to web server
4. start a consumer (first update the python-consumer.py) <---- 9:33p HERE
5. flume reads that and 'sinks it to Kafka...(which is already running-docker container, don't need start a producer)