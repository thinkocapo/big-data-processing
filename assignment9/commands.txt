"Type":"AWS::EC2::KeyPair::KeyName", 
I'M DOING 
Type":"AWS::EC2::KeyPair::hw1",
but ^ failed 'Unable to, External Values' so reverting to normal one
10:42p
next screen with lab9 name, asks for KeyName. done.

10:41p
lab9 is the name

10:50p
looks like where it's hosting my template (SetupKafka-AWS-PublicSubnets.json)
https://s3.us-east-2.amazonaws.com/cf-templates-1avazg83bnwg-us-east-2/2019308Ick-SetupKafka-AWS-PublicSubnets.json


10:53
ERROR
"Template error: Unable to get mapping for AmiId::us-east-2::AMI"

11:20p
switched to US West (N. California), it's working. user sug'd US East (Virginia) as well.
11:24P
COMPLETE or _IN_PROGRESS

11:25p
KAFKA BROKER ec2-X-X-X-X.us-west-1.compute.amazonaws.com
KAFKA ZOOKEE ec2-X-X-X-X.us-west-1.compute.amazonaws.com

cd /app/kafka/kafka_2.12-2.3.1
./bin/kafka-topics.sh --create --zookeeper <pri_vate_ip_address>:2181 --replication-factor 1 --partitions 2 --topic hw9events


12:16a
./bin/kafka-console.consumer.sh --bootstrap-server <PRIVATE_IP_OF_BROKER>:9092 --topic hw9events

12:29a
./bin/kafka-console-producer.sh --broker-list  <PRIVATE_IP_OF_BROKER>:9092 --topic hw9events


12:41A
NEW EC2 for producer.java:
sudo ssh -i ~/keypairs ec2-user@<t2_micro_ec2>.us-west-1.compute.amazonaws.com

#sudo scp -i ~/keypairs WebblogsKafkaProducer-jar-with-dependencies.jar ec2-user@<T2_MICRO_EC2>.us-west-1.compute.amazonaws.com:./
#sudo scp -i ~/keypairs WeblogsKafkaProducer.java ec2-user@<T2_MICRO_EC2>.us-west-1.compute.amazonaws.com:./
#java -cp WebblogsKafkaProducer-jar-with-dependencies.jar edu.harvard.e88.lab9.WeblogsKafkaProducer hw9events <T2_MICRO_EC2_IP>:9092
scp...python producer.py


11:03p
EC2 Cloud Formation
[running, no need consumer]

T2 EC2
python producer.py

EMR EC2:
~/opt/spark/bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0 $SparkStreamingConsumer.py localhost:9092 topicName <---localhost:9092 is the kafka broker URL that python accepts


11/5
ec2 t2 micro - producer.py with 1 to 100 events per second
>to<
ec2 kafka cluster (zookeeper+broker) - created topic on this. didn't have to run anyting else..?
>to<
ec2 emr Master - spark-submit consumer.py





by default takes from latest offset, in KafkaUtils.createDirectStream w kafkaParams  
https://spark.apache.org/docs/latest/streaming-programming-guide.html updateStateByKey *example*  
https://spark.apache.org/docs/2.1.0/api/python/pyspark.streaming.html  
rolling count



'''
by default takes from latest offset, in KafkaUtils.createDirectStream w kafkaParams
'''

'''
+configure your Spark Streaming job to read data from the "hw9events" Kafka topic,
+with the batch window of 1 sec
+implement a job that :
    +counts the number of clicks per URL per batch and prints the results after each batch
        counts the running total of clicks per URL and prints the results after each batch as well

DOCUMENTATION
https://spark.apache.org/docs/latest/streaming-programming-guide.html updateStateByKey *example*
https://spark.apache.org/docs/2.1.0/api/python/pyspark.streaming.html
rolling count
'''