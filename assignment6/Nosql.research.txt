Nosql research

https://medium.baqend.com/nosql-databases-a-survey-and-decision-guidance-ea7823a822d

KEY-VALUE STORE
Any assumptions about the structure of stored data are implicitly encoded in the application logic (schema-on-read) and not explicitly defined through a data definition language (schema-on-write).

WIDE-COLUMN
Wide-column stores inherit their name from the image that is often used to explain the underlying data model: a relational table with many sparse columns.

However, this storage layout usually enables highly efficient data compression and makes retrieving only a portion of an entity very efficient. The data are stored in lexicographic order of their keys, so that data that are accessed together are physically co-located, given a careful key design


SHARDING 3.1
Several distributed relational database systems such as Oracle RAC or IBM DB2 pureScale rely on a shared-disk architecture where all database nodes access the same central data repository (e.g. a NAS or SAN).

In contrast, the (NoSQL) database systems focused in this paper are built upon a shared-nothing architecture, meaning each system consists of many servers with private memory and private disks that are connected through a network. Thus, high scalability in throughput and data volume is achieved by sharding (partitioning) data across different nodes (shards) in the system. There are three basic distribution techniques: range-sharding, hash-sharding and entity-group sharding. 

range-sharding is good for scans
hash-sharding
The obvious disadvantage, though, is that it only allows lookups and makes scans unfeasible.

entity-group sharding
Entity-group sharding is a data partitioning scheme with the goal of enabling single-partition transactions on co-located data. The partitions are called entity-groups and either explicitly declared by the application (e.g. in G-Store andMegaStore) or derived from transactions’ access patterns (e.g. in Relational Cloud and Cloud SQL Server).


3.2 REPLICATION
*GOOD*
In terms of CAP, conventional RDBMSs are often CA systems run in single-server mode: The entire system becomes unavailable on machine failure. And so system operators secure data integrity and availability through expensive, but reliable high-end hardware. In contrast, NoSQL systems like Dynamo, BigTable or Cassandra are designed for data and request volumes that cannot possibly be handled by one single machine, and therefore they run on clusters consisting of thousands of servers. (Low-end hardware is used, because it is substantially more cost-efficient than high-end hardware.)

2-Tiers of Replication Strategies:
when+where

1st(when)
Eager(synchronous) replication propagates incoming changes synchronously to all replicas before a commit can be returned to the client, whereas 
Lazy (asynchronous) replication applies changes only at the receiving replica and passes them on asynchronously. The great advantage of eager replication is consistency among replicas, but it comes at the cost of higher write latency due to the need to wait for other replicas and impaired availability. Lazy replication is faster, because it allows replicas to diverge; as a consequence, stale data might be served.

2nd(where)
Either a
Master-slave (primary copy) scheme is pursued where changes can only be accepted by one replica (the master).
In master-slave protocols, concurrency control is not more complex than in a distributed system without replicas, but the entire replica set becomes unavailable, as soon as the master fails.

or, in a

Update anywhere (multi-master) approach, every replica can accept writes
 Multi-master protocols require complex mechanisms for prevention or detection and reconciliation of conflicting changes.


3.3 STORAGE MANAGEMENT
 *Storage management has a spatial dimension (where to store data) and a temporal dimension (when to store data)*

Update-in-place and append-only-IO are two complementary spatial techniques of organizing data; in-memory prescribes RAM as the location of data, whereas logging is a temporal technique that decouples main memory and persistent storage and thus provides control over when data is actually persisted.


*GOOD*ON*RDBMS*
In their seminal paper “the end of an architectural era”, Stonebraker et al. have found that in typical RDBMSs, only 6.8% of the execution time is spent on “useful work”, while the rest is spent on:
buffer management (34.6%), i.e. caching to mitigate slower disk access
latching (14.2%), to protect shared data structures from race conditions caused by multi-threading
locking (16.3%), to guarantee logical isolation of transactions
logging (1.9%), to ensure durability in the face of failures
hand-coded optimizations (16.2%)


This can be solved in two ways: The state can be replicated over n in-memory server nodes protecting against n-1 single-node failures (e.g. HStore, VoltDB) or by logging to durable storage (e.g. Redis or SAP Hana).

...Through logging, a random write access pattern can be transformed to a sequential one comprised of received operations and their associated properties (e.g. redo information). In most NoSQL systems, the commit rule for logging is respected, which demands every write operation that is confirmed as successful to be logged and the log to be flushed to persistent storage. In order to avoid the rotational latency of HDDs incurred by logging each operation individually, log flushes can be batched together (group commit) which slightly increases the latency of individual writes, but drastically improves throughput.
"log flushes"


3.4 QUERY PROCESSING
Primary key lookup, i.e. retrieving data items by a unique ID, is supported by every NoSQL system, since it is compatible to range- as well as hash-partitioning.

Filter queries return all items (or projections) that meet a predicate specified over the properties of data items from a single table

*YES*
To circumvent the inefficiencies of O(n) scans, secondary indexes can be employed. These can either be local secondary indexes that are managed in each partition or global secondary indexes that index data over all partitions. 


*RDBMS WINS FUNCTIONALITY*
*NOSQL WINS NON-FUNCTIONAL*
RDBMSs provide an unmatched level of functionality whereas NoSQL databases excel on the non-functional side through scalability, availability, low latency and/or high throughput.















-------
https://blog.matthewrathbone.com/2016/09/01/a-beginners-guide-to-hadoop-storage-formats.html

SEQUENCE.FILES
 They encode a key and a value for each record and nothing more. Records are stored in a binary format that is smaller than a text-based format would be. Like text files, the format does not encode the structure of the keys and values

 Sequence files by default use Hadoop’s Writable interface in order to figure out how to serialize and deserialize classes to the file.

Typically if you need to store complex data in a sequence file you do so in the value part while encoding the id in the key. The problem with this is that if you add or change fields in your Writable class it will not be backwards compatible with the data stored in the sequence file.

One benefit of sequence files is that they support block-level compression, so you can compress the contents of the file while also maintaining the ability to split the file into segments for multiple map tasks.

Sequence files are well supported across Hadoop and many other HDFS enabled projects, and I think represent the easiest next step away from text files.


AVRO
Honestly, Avro is not really a file format, it’s a file format plus a serialization and deserialization framework. 
then....
PARQUET:
The latest hotness in file formats for Hadoop is columnar file storage. Basically this means that instead of just storing rows of data adjacent to one another you also store column values adjacent to each other. So datasets are partitioned both horizontally and vertically.

This is particularly useful if your data processing framework just needs access to a subset of data that is stored on disk as it can access all values of a single column very quickly without reading whole records.

One huge benefit of columnar oriented file formats is that data in the same column tends to be compressed together which can yield some massive storage optimizations (as data in the same column tends to be similar).

If you’re chopping and cutting up datasets regularly then these formats can be very beneficial to the speed of your application, but frankly if you have an application that usually needs entire rows of data then the columnar formats may actually be a detriment to performance due to the increased network activity required.

Overall these formats can drastically optimize workloads, especially for Hive and Spark which tend to just read segments of records rather than the whole thing (which is more common in MapReduce).

Of the two file formats I mention, Parquet seems to have the most community support and is the format I would use.

Bonus: Compression Codecs


------------
https://blog.cloudera.com/the-small-files-problem/
A small file is one which is significantly smaller than the HDFS block size (default 64MB)

Every file, directory and block in HDFS is represented as an object in the namenode’s memory, each of which occupies 150 bytes, as a rule of thumb. So 10 million files, each using a block, would use about 3 gigabytes of memory. Scaling up much beyond this level is a problem with current hardware. Certainly a billion files is not feasible.

Furthermore, HDFS is not geared up to efficiently accessing small files: it is primarily designed for streaming access of large files. Reading through small files normally causes lots of seeks and lots of hopping from datanode to datanode to retrieve each small file, all of which is an inefficient data access pattern.

*SEQUENCE FILES*
The usual response to questions about “the small files problem” is: use a SequenceFile. The idea here is that you use the filename as the key and the file contents as the value. This works very well in practice. G

*HBASE*
'indexed SequenceFiles'
 HBase stores data in MapFiles (indexed SequenceFiles), and is a good choice if you need to do MapReduce style streaming analyses with the occasional random look up




---------
https://techmagie.wordpress.com/category/big-data/avro-parquet/

*WOAH*
Compressing data would speed up the I/O operations and would save storage space as well. But this could increase the processing time and CPU utilization because of decompression. So balance is required – more the compression – lesser is the data size but more the processing and CPU utilization.

*SO...*
Compressed files should also be splittable to support parallel processing.If a file is not splittable, it means we cannot input it to multiple tasks running in parallel and hence we lose the biggest advantage of parallel processing frameworks like Hadoop, Spark etc.


*GOOD POINT*
HDFS is good for scan type of queries but if random access to data is require then we should consider HBase.


*"SELF-DESCRIBING"*
Meta data management:
When data grows enormously, Meta data management becomes an important part of system. Data which is stored in HDFS it store din self-describing directory structure, which is also part of Meta data management. Typically when your data arrives it is tagged with source and arrival time and based upon these attributes it is also organized in HDFS in self-describing directory structure.


*TYPE-CONVERSION FOR LOTS OF VALUES GETS EXPENSIVE*
We should also consider the fact that when data is stored as text files, there would be additional overhead of type conversions. For e.g. storing 10000 as string would take up more space also would require type conversion from String to Int at the time of reading. This overhead grows considerably when we start processing TBs of data.


SEQUENCE FILES
These are mainly used as container for small files. Because storing many small files in HDFS could cause memory issues at NameNode and number of tasks created during processing would also be more, causing extra overhead.
Sequence files contains sync marker to distinguish between various blocks which makes it splittable. So now you can get splittability with non splitable compression format like Snappy. You can compress the individual blocks and retaining the splitable nature using sync markers.


*AVRO HAS SCHEMA!!!!!*
Avro is language neutral data serialization
Writables has the drawback that they do not provide language portability.
Avro formatted data can be described through language independent schema. Hence Avro formatted data can be shared across applications using different languages.
Avro stores the schema in header of file so data is self-describing.
Avro formatted files are splittable and compressible and hence it’s a good candidate for data storage in Hadoop ecosystem.
Schema Evolution – Schema used to read a Avro file need not be same as schema which was used to write the files. This makes it possible to add new fields.


Just as with Sequence Files, Avro files also contains Sync markers to separate the blocks. This makes it splittable.
These blocks can be compressed using compression formats such Snappy and Deflate.
Hive provides the inbuilt SerDe (AvroSerDe) to read-write data in table.
Supported data types – int, boolean, float, string, double, map, array complex data types, nested data types etc.


*SIMILAR*
Avro and Thrift and Protobuf
Avro more popular here




COLUMNAR FORMATS
Yes - *They eliminate I/O for columns that are not part of query. So works well for queries which require only subset of columns.
Provides better compression as similar data is grouped together in columnar format.*
Parquet

*PARQUET CAN BE READ INTO AVRO SCHEMA!!!!*
Parquet can be read and write using Avro API and Avro Schema.

*LIKE*
Object model is in memory representation of data. In Parquet it is possible to change the Object model to Avro which gives a rich object model.


"projection pushdown" and "predicate pushdown" ?






