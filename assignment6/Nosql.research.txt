Nosql research

https://medium.baqend.com/nosql-databases-a-survey-and-decision-guidance-ea7823a822d

KEY-VALUE STORE
Any assumptions about the structure of stored data are implicitly encoded in the application logic (schema-on-read) and not explicitly defined through a data definition language (schema-on-write).

WIDE-COLUMN
Wide-column stores inherit their name from the image that is often used to explain the underlying data model: a relational table with many sparse columns.

However, this storage layout usually enables highly efficient data compression and makes retrieving only a portion of an entity very efficient. The data are stored in lexicographic order of their keys, so that data that are accessed together are physically co-located, given a careful key design


SHARDING 3.1
Several distributed relational database systems such as Oracle RAC or IBM DB2 pureScale rely on a shared-disk architecture where all database nodes access the same central data repository (e.g. a NAS or SAN).

In contrast, the (NoSQL) database systems focused in this paper are built upon a shared-nothing architecture, meaning each system consists of many servers with private memory and private disks that are connected through a network. Thus, high scalability in throughput and data volume is achieved by sharding (partitioning) data across different nodes (shards) in the system. There are three basic distribution techniques: range-sharding, hash-sharding and entity-group sharding. 

range-sharding is good for scans
hash-sharding
The obvious disadvantage, though, is that it only allows lookups and makes scans unfeasible.

entity-group sharding
Entity-group sharding is a data partitioning scheme with the goal of enabling single-partition transactions on co-located data. The partitions are called entity-groups and either explicitly declared by the application (e.g. in G-Store andMegaStore) or derived from transactions’ access patterns (e.g. in Relational Cloud and Cloud SQL Server).


3.2 REPLICATION
*GOOD*
In terms of CAP, conventional RDBMSs are often CA systems run in single-server mode: The entire system becomes unavailable on machine failure. And so system operators secure data integrity and availability through expensive, but reliable high-end hardware. In contrast, NoSQL systems like Dynamo, BigTable or Cassandra are designed for data and request volumes that cannot possibly be handled by one single machine, and therefore they run on clusters consisting of thousands of servers. (Low-end hardware is used, because it is substantially more cost-efficient than high-end hardware.)

2-Tiers of Replication Strategies:
when+where

1st(when)
Eager(synchronous) replication propagates incoming changes synchronously to all replicas before a commit can be returned to the client, whereas 
Lazy (asynchronous) replication applies changes only at the receiving replica and passes them on asynchronously. The great advantage of eager replication is consistency among replicas, but it comes at the cost of higher write latency due to the need to wait for other replicas and impaired availability. Lazy replication is faster, because it allows replicas to diverge; as a consequence, stale data might be served.

2nd(where)
Either a
Master-slave (primary copy) scheme is pursued where changes can only be accepted by one replica (the master).
In master-slave protocols, concurrency control is not more complex than in a distributed system without replicas, but the entire replica set becomes unavailable, as soon as the master fails.

or, in a

Update anywhere (multi-master) approach, every replica can accept writes
 Multi-master protocols require complex mechanisms for prevention or detection and reconciliation of conflicting changes.


3.3 STORAGE MANAGEMENT
 *Storage management has a spatial dimension (where to store data) and a temporal dimension (when to store data)*

Update-in-place and append-only-IO are two complementary spatial techniques of organizing data; in-memory prescribes RAM as the location of data, whereas logging is a temporal technique that decouples main memory and persistent storage and thus provides control over when data is actually persisted.


*GOOD*ON*RDBMS*
In their seminal paper “the end of an architectural era”, Stonebraker et al. have found that in typical RDBMSs, only 6.8% of the execution time is spent on “useful work”, while the rest is spent on:
buffer management (34.6%), i.e. caching to mitigate slower disk access
latching (14.2%), to protect shared data structures from race conditions caused by multi-threading
locking (16.3%), to guarantee logical isolation of transactions
logging (1.9%), to ensure durability in the face of failures
hand-coded optimizations (16.2%)


This can be solved in two ways: The state can be replicated over n in-memory server nodes protecting against n-1 single-node failures (e.g. HStore, VoltDB) or by logging to durable storage (e.g. Redis or SAP Hana).

...Through logging, a random write access pattern can be transformed to a sequential one comprised of received operations and their associated properties (e.g. redo information). In most NoSQL systems, the commit rule for logging is respected, which demands every write operation that is confirmed as successful to be logged and the log to be flushed to persistent storage. In order to avoid the rotational latency of HDDs incurred by logging each operation individually, log flushes can be batched together (group commit) which slightly increases the latency of individual writes, but drastically improves throughput.
"log flushes"


3.4 QUERY PROCESSING
Primary key lookup, i.e. retrieving data items by a unique ID, is supported by every NoSQL system, since it is compatible to range- as well as hash-partitioning.

Filter queries return all items (or projections) that meet a predicate specified over the properties of data items from a single table

*YES*
To circumvent the inefficiencies of O(n) scans, secondary indexes can be employed. These can either be local secondary indexes that are managed in each partition or global secondary indexes that index data over all partitions. 


*RDBMS WINS FUNCTIONALITY*
*NOSQL WINS NON-FUNCTIONAL*
RDBMSs provide an unmatched level of functionality whereas NoSQL databases excel on the non-functional side through scalability, availability, low latency and/or high throughput.















-------
https://blog.matthewrathbone.com/2016/09/01/a-beginners-guide-to-hadoop-storage-formats.html

SEQUENCE.FILES
 They encode a key and a value for each record and nothing more. Records are stored in a binary format that is smaller than a text-based format would be. Like text files, the format does not encode the structure of the keys and values

 Sequence files by default use Hadoop’s Writable interface in order to figure out how to serialize and deserialize classes to the file.

Typically if you need to store complex data in a sequence file you do so in the value part while encoding the id in the key. The problem with this is that if you add or change fields in your Writable class it will not be backwards compatible with the data stored in the sequence file.

One benefit of sequence files is that they support block-level compression, so you can compress the contents of the file while also maintaining the ability to split the file into segments for multiple map tasks.

Sequence files are well supported across Hadoop and many other HDFS enabled projects, and I think represent the easiest next step away from text files.


AVRO
Honestly, Avro is not really a file format, it’s a file format plus a serialization and deserialization framework. 
then....
PARQUET:
The latest hotness in file formats for Hadoop is columnar file storage. Basically this means that instead of just storing rows of data adjacent to one another you also store column values adjacent to each other. So datasets are partitioned both horizontally and vertically.

This is particularly useful if your data processing framework just needs access to a subset of data that is stored on disk as it can access all values of a single column very quickly without reading whole records.

One huge benefit of columnar oriented file formats is that data in the same column tends to be compressed together which can yield some massive storage optimizations (as data in the same column tends to be similar).

If you’re chopping and cutting up datasets regularly then these formats can be very beneficial to the speed of your application, but frankly if you have an application that usually needs entire rows of data then the columnar formats may actually be a detriment to performance due to the increased network activity required.

Overall these formats can drastically optimize workloads, especially for Hive and Spark which tend to just read segments of records rather than the whole thing (which is more common in MapReduce).

Of the two file formats I mention, Parquet seems to have the most community support and is the format I would use.

Bonus: Compression Codecs


------------
https://blog.cloudera.com/the-small-files-problem/
A small file is one which is significantly smaller than the HDFS block size (default 64MB)

Every file, directory and block in HDFS is represented as an object in the namenode’s memory, each of which occupies 150 bytes, as a rule of thumb. So 10 million files, each using a block, would use about 3 gigabytes of memory. Scaling up much beyond this level is a problem with current hardware. Certainly a billion files is not feasible.

Furthermore, HDFS is not geared up to efficiently accessing small files: it is primarily designed for streaming access of large files. Reading through small files normally causes lots of seeks and lots of hopping from datanode to datanode to retrieve each small file, all of which is an inefficient data access pattern.

*SEQUENCE FILES*
The usual response to questions about “the small files problem” is: use a SequenceFile. The idea here is that you use the filename as the key and the file contents as the value. This works very well in practice. G

*HBASE*
'indexed SequenceFiles'
 HBase stores data in MapFiles (indexed SequenceFiles), and is a good choice if you need to do MapReduce style streaming analyses with the occasional random look up




---------
https://techmagie.wordpress.com/category/big-data/avro-parquet/

*WOAH*
Compressing data would speed up the I/O operations and would save storage space as well. But this could increase the processing time and CPU utilization because of decompression. So balance is required – more the compression – lesser is the data size but more the processing and CPU utilization.

*SO...*
Compressed files should also be splittable to support parallel processing.If a file is not splittable, it means we cannot input it to multiple tasks running in parallel and hence we lose the biggest advantage of parallel processing frameworks like Hadoop, Spark etc.


*GOOD POINT*
HDFS is good for scan type of queries but if random access to data is require then we should consider HBase.


*"SELF-DESCRIBING"*
Meta data management:
When data grows enormously, Meta data management becomes an important part of system. Data which is stored in HDFS it store din self-describing directory structure, which is also part of Meta data management. Typically when your data arrives it is tagged with source and arrival time and based upon these attributes it is also organized in HDFS in self-describing directory structure.


*TYPE-CONVERSION FOR LOTS OF VALUES GETS EXPENSIVE*
We should also consider the fact that when data is stored as text files, there would be additional overhead of type conversions. For e.g. storing 10000 as string would take up more space also would require type conversion from String to Int at the time of reading. This overhead grows considerably when we start processing TBs of data.


SEQUENCE FILES
These are mainly used as container for small files. Because storing many small files in HDFS could cause memory issues at NameNode and number of tasks created during processing would also be more, causing extra overhead.
Sequence files contains sync marker to distinguish between various blocks which makes it splittable. So now you can get splittability with non splitable compression format like Snappy. You can compress the individual blocks and retaining the splitable nature using sync markers.


*AVRO HAS SCHEMA!!!!!*
Avro is language neutral data serialization
Writables has the drawback that they do not provide language portability.
Avro formatted data can be described through language independent schema. Hence Avro formatted data can be shared across applications using different languages.
Avro stores the schema in header of file so data is self-describing.
Avro formatted files are splittable and compressible and hence it’s a good candidate for data storage in Hadoop ecosystem.
Schema Evolution – Schema used to read a Avro file need not be same as schema which was used to write the files. This makes it possible to add new fields.


Just as with Sequence Files, Avro files also contains Sync markers to separate the blocks. This makes it splittable.
These blocks can be compressed using compression formats such Snappy and Deflate.
Hive provides the inbuilt SerDe (AvroSerDe) to read-write data in table.
Supported data types – int, boolean, float, string, double, map, array complex data types, nested data types etc.


*SIMILAR*
Avro and Thrift and Protobuf
Avro more popular here




COLUMNAR FORMATS
Yes - *They eliminate I/O for columns that are not part of query. So works well for queries which require only subset of columns.
Provides better compression as similar data is grouped together in columnar format.*
Parquet

*PARQUET CAN BE READ INTO AVRO SCHEMA!!!!*
Parquet can be read and write using Avro API and Avro Schema.

*LIKE*
Object model is in memory representation of data. In Parquet it is possible to change the Object model to Avro which gives a rich object model.


"projection pushdown" and "predicate pushdown" ?



------------------------
comparing-database-types-How Databases Evolved to Meet Different Needs _ Prisma.pdf

"FLATFILE"
The simplest way to manage data on a computer outside of an application is to store it in a basic file format. The first solutions for data management used this approach and it is still a popular option for storing small amounts of information without heavy requirements.
The first flat file databases represented information in regular, machine parse-able structures within files. Data is stored in plain text, which limits the type of content that can be represented within the database itself. 

"HIERARCHICAL"
This simple relationship mapping provides users with the ability to establish relationships between items in a tree structure. This is very useful for certain types of data, but does not allow for complex relationship management. Furthermore, the meaning of the parent-child relationship is implicit. One parent-child connection could be between a customer and their orders, while another might represent an employee and the equipment they have been allocated. The structure of the data itself does not distinguish between these relationships.

Hierarchical databases are the beginning of a movement towards thinking about data management in more complex terms. The trajectory of database management systems that were developed afterwards continues this trend.


"NETWORK DATABASES"
"entries can have more than one parent" so more complex relationships
the 'network' refers to connections between different data entries (not connections between different computers or software)
represented by a generic graph instead of a tree.
"the meeting of graph was defined by a schema"
Network databases were a huge leap forward in terms of flexibility and the ability to map connections between information. However, they were still limited by the same access patterns and design mindset of hierarchical databases

"RELATIONAL DATABASES" RDBMS
tables
'while SQL is not a part of the relational system, it is often a fundamental part of working with these databases.'
'Because relational databases work off of a schema, it can be more challenging to alter the structure of data after it is in the system. 
However, the schema also helps enforce the integrity of the data, making sure values match the expected formats, and that required information is included.'

YESSS
If key-value stores appear simple, it's because they are. But that simplicity is often an asset in the kinds of scenarios where they are most often deployed. Key-value stores are often used to store configuration data, state information, and any data that might be represented by a dictionary or hash in a programming language. Key-value stores provide fast, low-complexity access to this type of data

COOOLLLLL
One of the most popular uses for key-value databases are to store configuration values and application variables and flags for websites and web applications. Programs can check the key-value store, which is usually very fast, for their configuration when they start. This allows you to alter the runtime behavior of your services by changing the data in the key-value store. Applications can also be configured to recheck periodically or to
 https://www.prisma.io/blog/comparison-of-database-models-1iz9u29nwn37 9/20
9/24/2019 Comparing Database Types: How Database Types Evolved to Meet Different Needs | Prisma
restart when they see changes. These configuration stores are often persisted to disk periodically to prevent loss of data in the event of a system crash.
Examples: Redis, memcached, etcd


Graph databases: mapping relationships by focusing on how connections between data are meaningful
...
 graph databases establish connections using the concepts of nodes, edges, and properties.

At a glance, graph databases appear similar to earlier network databases

'instead of relationships between tables' its about 'managing relationships themselves'


For example, querying for the connection between two users of a social media site in a relational database is likely to require multiple table joins and therefore be rather resource intensive. This same query would be straightforward in a graph database that directly maps connections. The focus of graph databases is to make working this type of data intuitive and powerful.
• Neo4j, Titan



Instead of tables, column-family databases have structures called column families.
column-family store, column-family database

YESS
With this design, each row in a column family defines its own schema. That schema can be easily modified because it only affects that single row of data. Each row can have different numbers of columns with different types of data. Sometimes it helps to think of column family databases as key-value databases where each key (row identifier) returns a dictionary of arbitrary attributes and their values (the column names and their values).


1
Column-family databases are good when working with applications that requires great performance for row-based operations and highly scalability.
2
 Since all of the data and metadata for an entry is accessible with a single row identifier, no computationally expensive joins are required to find and pull the information. 
3
The database system also typically makes sure all of the data in a row is collocated on the same machine in a cluster, simplifying data sharding and scaling.


WARNN....
"However, column-family databases do not work well in all scenarios. If you have highly relational data that requires joins, this is not the right type of database for your application. Column-family databases are firmly oriented around row-based operations. This means that aggregate queries like summing, averaging, and other analytics- oriented processes can be difficult or impossible. This can have a great impact on how you design your applications and what types of usage patterns you can use."



 The goal is to offer greater scalability than relational databases and greater consistency guarantees than NoSQL alternatives.
...
 They achieve this by sacrificing certain amounts of availability in the event of a networking partition.

 *GOLD RULE / QUESTION*
 "It asserts that in the event of a network partition, a distributed database can choose either to remain available or remain consistent, but it cannot do both. "


For instance, splitting data sets into smaller ranges called shards can minimize the amount of data that is unavailable during partitions.


??????????????
Because of these qualities, NewSQL databases are best suited for use cases with high volumes of relational data in distributed, cloud-like environments.
While NewSQL databases offer most of the familiar features of conventional relational databases, there are some important differences that prevent it from being a one-to- one replacement. NewSQL systems are typically less flexible and generalized than their more conventional relational counterparts. They also usually only offer a subset of full SQL and relational features, which means that they might not be able to handle certain kinds of usage. Many NewSQL implementations also store a large part of or their entire dataset in the computer's main memory. This improves performance at the cost of greater risk to unpersisted changes.




